{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Processor.Preprocessor import PreProcessing\n",
    "from Model.AnomalyTransformer import AnomalyTransformer\n",
    "from Model.EarlyStopping import EarlyStopping\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = ''\n",
    "train_dataloader,valid_dataloader,test_dataloader = PreProcessing(Path = Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337add09",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "lambda_value = 3\n",
    "window = 100\n",
    "model = AnomalyTransformer(win_size = window, e_layers=6,enc_in = 4, c_out = 4).to(device)\n",
    "es = EarlyStopping()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b82d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"======================TRAIN MODE======================\")\n",
    "\n",
    "\n",
    "\n",
    "def my_kl_loss(p, q):\n",
    "    res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
    "    return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
    "\n",
    "win_size=window\n",
    "k = lambda_value\n",
    "\n",
    "time_now = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    iter_count = 0\n",
    "    loss1_list = []\n",
    "    valid_list = []\n",
    "    \n",
    "    epoch_time = time.time()\n",
    "    #train\n",
    "    model.train()\n",
    "    for i, (input_data,_) in enumerate(train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        iter_count += 1\n",
    "        input = input_data.float().to(device)\n",
    "    \n",
    "        output, series, prior, _ = model(input)\n",
    "\n",
    "        # calculate Association discrepancy\n",
    "        series_loss = 0.0\n",
    "        prior_loss = 0.0\n",
    "        for u in range(len(prior)):\n",
    "            series_loss += (torch.mean(my_kl_loss(series[u], (\n",
    "                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                           win_size)).detach())) + torch.mean(\n",
    "                my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   win_size)).detach(),\n",
    "                           series[u])))\n",
    "            prior_loss += (torch.mean(my_kl_loss(\n",
    "                (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                        win_size)),\n",
    "                series[u].detach())) + torch.mean(\n",
    "                my_kl_loss(series[u].detach(), (\n",
    "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                               win_size)))))\n",
    "        series_loss = series_loss / len(prior)\n",
    "        prior_loss = prior_loss / len(prior)\n",
    "\n",
    "        rec_loss = criterion(output, input)\n",
    "\n",
    "        loss1_list.append((rec_loss - k * series_loss).item())\n",
    "        loss1 = rec_loss - k * series_loss\n",
    "        loss2 = rec_loss + k * prior_loss\n",
    "        \n",
    "        \n",
    "        # Minimax strategy\n",
    "        loss1.backward(retain_graph=True)\n",
    "        loss2.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #valid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_data,_) in enumerate(valid_dataloader):\n",
    "            input = input_data.float().to(device)\n",
    "            output, series, prior, _ = model(input)\n",
    "\n",
    "            # calculate Association discrepancy\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                series_loss += (torch.mean(my_kl_loss(series[u], (\n",
    "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                               win_size)).detach())) + torch.mean(\n",
    "                    my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                       win_size)).detach(),\n",
    "                               series[u])))\n",
    "                prior_loss += (torch.mean(my_kl_loss(\n",
    "                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                            win_size)),\n",
    "                    series[u].detach())) + torch.mean(\n",
    "                    my_kl_loss(series[u].detach(), (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   win_size)))))\n",
    "            series_loss = series_loss / len(prior)\n",
    "            prior_loss = prior_loss / len(prior)\n",
    "\n",
    "            rec_loss = criterion(output, input)\n",
    "\n",
    "            valid_list.append((rec_loss - k * series_loss).item())\n",
    "            valid_loss = rec_loss - k * series_loss\n",
    "\n",
    "        \n",
    "    train_loss = np.average(loss1_list)\n",
    "    es(valid_loss,model)\n",
    "    if es.early_stop:\n",
    "        print('Early Stopping')\n",
    "        break\n",
    "    print(f\"Epoch: {epoch + 1} | Train Loss: {train_loss:.4f} | Valid Loss {valid_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88160d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_test(dataloader):\n",
    "    temperature = 50\n",
    "    test_energy = []\n",
    "    test_labels = []\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    win_size = 100\n",
    "\n",
    "    for i, (input_data,labels) in enumerate(dataloader):\n",
    "        input = input_data.float().to(device)\n",
    "        output, series, prior, _ = model(input)\n",
    "        loss = torch.mean(criterion(input, output), dim=-1)\n",
    "        \n",
    "        series_loss = 0.0\n",
    "        prior_loss = 0.0\n",
    "        for u in range(len(prior)):\n",
    "            if u == 0:\n",
    "                series_loss = my_kl_loss(series[u], (\n",
    "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                               win_size)).detach()) * temperature\n",
    "                prior_loss = my_kl_loss(\n",
    "                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                            win_size)),\n",
    "                    series[u].detach()) * temperature\n",
    "            else:\n",
    "                series_loss += my_kl_loss(series[u], (\n",
    "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                               win_size)).detach()) * temperature\n",
    "                prior_loss += my_kl_loss(\n",
    "                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                            win_size)),\n",
    "                    series[u].detach()) * temperature\n",
    "        \n",
    "        metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
    "        cri = metric * loss\n",
    "        cri = cri.detach().cpu().numpy()\n",
    "        \n",
    "        test_energy.append(cri)\n",
    "        test_labels.append(labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    test_energy = np.concatenate(test_energy, axis=0).reshape(-1)\n",
    "    test_energy = np.array(test_energy)\n",
    "    test_labels = np.concatenate(test_labels, axis=0).reshape(-1)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return test_energy, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eg, test_la= anomaly_test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2636aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_value(list1):\n",
    "    list1 = list1.reshape(-1,100)\n",
    "    max_value_list=[]\n",
    "    for i in range(len(list1)):\n",
    "        max_value= max(list1[i])\n",
    "        max_value_list.append(max_value)\n",
    "    max_value_list = np.array(max_value_list)\n",
    "    return max_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_test = max_value(test_eg)\n",
    "mv_tela = np.concatenate((mv_test.reshape(-1,1),test_la.reshape(-1,1)), axis = 1)\n",
    "mv_tela = pd.DataFrame(mv_tela,columns = ['mv','la'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed16ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eg,test_la = anomaly_test(train_dataloader)\n",
    "mv_train = max_value(train_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.percentile(mv_tela.mv[mv_tela.la==0], 25)\n",
    "Q3 = np.percentile(mv_tela.mv[mv_tela.la==0], 75)\n",
    "IQR = Q3 -Q1\n",
    "threshold = Q3 + (1.0 * IQR)\n",
    "print(f'Th : {threshold:.5f} | Q1 : {Q1:.5f} Q2 : {Q3:.5f} IQR : {IQR:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for v,la in zip(mv_tela.mv,mv_tela.la):\n",
    "    if v < threshold :\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        for v in mv_tela[mv_tela.la==i].mv:\n",
    "            y_true.append(i)\n",
    "            if v <= threshold:\n",
    "                y_pred.append(i)\n",
    "                \n",
    "            else :\n",
    "                y_pred.append(2)\n",
    "    else:\n",
    "        for v in mv_tela[mv_tela.la==i].mv:\n",
    "            y_true.append(i)\n",
    "            if v >= threshold:\n",
    "                y_pred.append(i)\n",
    "            else:\n",
    "                y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot = True, cmap = 'Blues',fmt='.0f')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Anomaly Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=0\n",
    "TP=0\n",
    "FP=0\n",
    "FN=0\n",
    "for pr,la in zip(pred,mv_tela.la):\n",
    "    if pr == 0 and la == 0:\n",
    "        TN += 1\n",
    "    elif pr != 0 and la != 0:\n",
    "        TP += 1\n",
    "    elif pr != 0 and la == 0:\n",
    "        FP += 1\n",
    "    else:\n",
    "        FN +=1\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "f1 = 2/((1/precision)+(1/recall))\n",
    "print(f'precision : {precision*100:.3f}')\n",
    "print(f'recall : {recall*100:.3f}')\n",
    "print(f'F1-score : {f1*100:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
